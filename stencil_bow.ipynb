{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ncai25/ncai25/blob/main/stencil_bow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIC7W3qEZy6o"
      },
      "source": [
        "# ðŸ’°ï¸Bag-of-Words Model for Sentiment Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGoidRLpZy6w"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb7FP8sBZy6w"
      },
      "source": [
        "Welcome to your first assignment! In this assignment, you will be implementing your very first NLP model (woohoo!) - a bag-of-words (BOW) model for sentiment classification. You will be using a very useful NLP libary (spaCy), and a popular machine learning library (scikit-learn, or sklearn) to help you with preprocessing the data and building the BOW model. In later lectures and assignments, you will learn to implement many of these NLP and machine learning functions yourself, but in \"the real world\" it is often more practical to use existing libraries, like those provided by spaCy and sklearn, than to implement these algorithms from scratch. So here we want you to learn how to use these libraries in a practical setting, and we'll learn later more about how they work \"under the hood\".\n",
        "\n",
        "The main learning objectives for this assignment are:\n",
        "\n",
        "1. Use sklearn to implement a standard ML workflow (featurization, designing train-test splits, training a model, and evaluating the model).\n",
        "2. Use spaCy for standard NLP preprocessing steps (tokenization, lemmatization, tagging).\n",
        "3. Understand and implement common featurization approaches for text classifiers (ngrams, weighting schemas, tagging).\n",
        "\n",
        "The basic workflow for an NLP model contains the following steps:\n",
        "\n",
        "1. Load and explore the data\n",
        "2. Preprocess the data\n",
        "3. Extract features\n",
        "4. Train the model\n",
        "5. Evaluate the model\n",
        "6. Analyze model behavior\n",
        "\n",
        "You will implement this full workflow in Part 1. In Part 2, you will experiment with improvements to step 2 (preprocessing). In Part 3, you will experiment with improvements to step 3 (featurization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tESbIOp4Zy6y"
      },
      "source": [
        "## Before you start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7JcSXneZy6z"
      },
      "source": [
        "In this class, we will be using Google Colab notebooks for assignments instead of regular Python files. Colab provides more efficient Google GPUs and is easier for setting up. Colab is also a very powerful platform and it functions very similarly to a Jupyter notebook. We'll provide a few tips on how to use Colab below; make sure you read them carefully!\n",
        "\n",
        "â˜ **Quick Colab Tips:**\n",
        "\n",
        "- Make sure you **create a copy** of the Colab file before you start coding!! (File > Save a copy in Drive). If you don't do this, you will lose everything when you close the page and we DON'T want that to happen to you!\n",
        "- To run a cell, press the play button in its top left corner. Make sure you run all previous cells before running the next one, so that every function and variable you need is defined.\n",
        "- If you don't want to run each cell individually, you can click Runtime and you will find \"Run Before\", \"Run After\", or \"Run All\".\n",
        "- When you reload the page, the runtime restarts and all variables in the environment are cleared. This means that you will need to re-run cells.\n",
        "- If you make changes to a definition in an earlier cell, remember to run the cell to actually update the definition. Then remember to re-run all the cells after using that definition, or else they will still be using the previous value.\n",
        "- For more tips on getting started, please take a look at this [video](https://www.youtube.com/watch?v=inN8seMm7UI&ab_channel=TensorFlow)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnKGrey8cZp2"
      },
      "source": [
        "## Installation and Imports\n",
        "â˜ **Quick Tip:** You can add \"**!**\" in front a Linux command and it can run on Colab!\n",
        "\n",
        "The following code will ask to access your Google Drive; you will be downloading the dataset used in this assignment to your Google Drive, and uploading it via this access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXyZnyPJc2Ze"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install scikit-learn\n",
        "\n",
        "import os\n",
        "import spacy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from typing import TypeAlias\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAgJ7cm-nUJ0"
      },
      "source": [
        "# Part 1: BOW Workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LFmmfC40yvK"
      },
      "source": [
        "### Step 1: Load the raw data and explore it\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgkVdcg-g_A2"
      },
      "source": [
        "In this assignment, we'll be using a dataset of tweets pulled from Twitter. Download the dataset [from Google Drive here](https://drive.google.com/file/d/1Rx-HWAWFVD5oJjIQL5u0fxvbPrBnLwmJ/view?usp=sharing). Add it to your own Google Drive and change the `FOLDER` variable below to reflect its location. (Generally it should start with \"/content\\/drive\\/My Drive\". You can click on the file icon on the bar on the left to see how your Drive has been mounted. Click the three dots to the right of the folder and \"Copy path\" to get the path to your folder.)\n",
        "\n",
        "In the dataset, the tweets are classified according to three categories of sentiment: positive, negative and neutral. We will use the tweets in the `text` column and the sentiments in the `sentiment` column of the CSV file. Feel free to read the Pandas documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) if you are unfamiliar with Pandas Dataframes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruKcSLB7qmV1"
      },
      "outputs": [],
      "source": [
        "FOLDER = \"/content/drive/My Drive/...\"\n",
        "FILEPATH = f\"{FOLDER}/Tweets_5K.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szjiUpBdl1Ub"
      },
      "outputs": [],
      "source": [
        "def load_data(filepath: str) -> tuple[list[str], list[int]]:\n",
        "    \"\"\"\n",
        "    Loads Twitter data into two lists.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    raw_tweets : list[str]\n",
        "        A list of all Tweets in the dataset\n",
        "    labels : list[int]\n",
        "        A list of the sentiments corresponding to each raw tweet encoded as integers,\n",
        "        -1 meaning negative, 0 meaning neutral, and 1 meaning positive\n",
        "    \"\"\"\n",
        "    # TODO: Load data from the CSV file using filepath\n",
        "    # Remember to map positive tweets to 1, neutral tweets to 0, and negative tweets to -1.\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79FM4wqYFDUn"
      },
      "source": [
        "Before you start to preprocess data, you should always take a look at the dataset and get a sense of the data you're handling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuXYtIARFZ_I"
      },
      "outputs": [],
      "source": [
        "raw_tweets, labels = load_data(FILEPATH)\n",
        "for p, label in zip(raw_tweets[:10], labels[:10]):\n",
        "    print(f\"{label}:\\t{p}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EznlXSqYRwUd"
      },
      "source": [
        "We can use matplotlib to see the ratio between the number of positive, negative and neutral tweets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbL8lmGqR4W1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "pd.Series(labels).value_counts().plot.bar(title=\"Sentiment Distribution in Tweets\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Number of Tweets\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHTnM_KmhSX4"
      },
      "source": [
        "### Step 2: Preprocess the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpaUKiGIR0O9"
      },
      "source": [
        "For now, all you need to do for preprocessing is split the tweets by whitespace.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS_vaKZYhW8p"
      },
      "outputs": [],
      "source": [
        "def preprocess(raw_X: list[str]) -> list[list[str]]:\n",
        "    \"\"\"\n",
        "    Performs splitting on whitespace on all raw strings in a list.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    raw_X : list[str]\n",
        "        A list of raw strings (tweets)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list[list[str]]\n",
        "        A list of preprocessed tweets (which are now lists of words)\n",
        "    \"\"\"\n",
        "    # TODO Basic tokenization just based on whitespace, with no other preprocessing\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2hYek_ZiEG8"
      },
      "source": [
        "### Steps 3 & 4: Define featurization + train and test functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0hIpa31iFcV"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "class BOW_Classifier:\n",
        "    \"\"\"\n",
        "    Attributes\n",
        "    ----------\n",
        "    clf : LogisticRegression\n",
        "        A logistic regression classifier\n",
        "    dv : DictVectorizer\n",
        "        A dictionary vectorizer for turning dictionaries into matrices\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # TODO: Initialize class attributes\n",
        "        # HINT: Use max_iter=150 when initializing your LogisticRegression classifier to get expected performance!\n",
        "        # You are allowed to ignore the DictVectorizer dv attribute, as well as add more to the init method, if you wish\n",
        "        self.clf = ...\n",
        "        self.dv = ...\n",
        "\n",
        "    def featurize(self, preproc_X: np.ndarray[list[str]], is_test: bool = False) -> csr_matrix:\n",
        "        \"\"\"\n",
        "        Turns a list of preprocessed tweets into a binary bag-of-words matrix.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        preproc_X : np.ndarray[list[str]]\n",
        "            A list of preprocessed tweets\n",
        "        is_test: bool, default=False\n",
        "            Whether featurization should be done using features learned during training (is_test=True)\n",
        "            or whether it should be done with features extracted from scratch using preproc_X (is_test=False)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        csr_matrix\n",
        "            A matrix with rows corresponding to tweets and columns corresponding to words\n",
        "        \"\"\"\n",
        "        # TODO: Implement a binary bag-of-words for unigrams\n",
        "        pass\n",
        "\n",
        "    def train(self, X_train: np.ndarray[list[str]], y_train: np.ndarray[int]):\n",
        "        \"\"\"\n",
        "        Trains the BOW classifier on the given training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_train : np.ndarray[list[str]]\n",
        "            Preprocessed tweets for training\n",
        "        y_train : np.ndarray[int]\n",
        "            Sentiments corresponding to the tweets in X_train\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        pass\n",
        "\n",
        "    def test(self, X_test: np.ndarray[list[str]]) -> np.ndarray[int]:\n",
        "        \"\"\"\n",
        "        Classifies the given test data and returns predicted sentiments.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_test : np.ndarray[list[str]]\n",
        "            Preprocessed tweets for testing\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : np.ndarray[int]\n",
        "            Predicted sentiments for the tweets in X_test\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0WMMAoGjlFT"
      },
      "source": [
        "### Step 5: Evaluate the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgqYc0tqjpkr"
      },
      "source": [
        "To evaluate the model, we need to simulate the \"real world\" setting in which we have trained on our model on the data we have, but now we are using it to assign labels to data we have never seen before. We will do this using stratified k-fold cross validation.\n",
        "\n",
        "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called `k` that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. This procedure is called \"stratified\" when the data is divided so that examples from each class (in this case, the classes are the three sentiments) are distributed evenly among all folds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVS-vcP2joXG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def run_kfold_crossval(\n",
        "    model: BOW_Classifier, X: list[list[str]], y: list[int], k: int = 5\n",
        ") -> list[float]:\n",
        "    \"\"\"\n",
        "    Executes k-fold cross-validation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : BOW_Classifier\n",
        "        A BOW model that has train and test methods\n",
        "    X : list[list[str]]\n",
        "        Preprocessed tweets for training and testing\n",
        "    y : list[int]\n",
        "        Sentiments corresponding to the tweets in X\n",
        "    k : int, default=5\n",
        "        The number of folds to use for cross-validation\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list[float]\n",
        "        A list of accuracy values from testing with each fold\n",
        "    \"\"\"\n",
        "    # TODO: Implement stratified k-fold cross-validation with a BOW_Classifier model\n",
        "    # HINT: Consult the documentation for the imports at the top of this cell\n",
        "    # HINT 2: If you have trouble casting a ragged nested list to a numpy array, try passing dtype=list\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOz9M-b8laaq"
      },
      "source": [
        "### Step 6: Plotting model performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wddNvm-Rlb7M"
      },
      "source": [
        "This function will plot how well our model is doing. :)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3Q5XsZglda0"
      },
      "outputs": [],
      "source": [
        "def plot_perfs(perfs: list[list[float]], names: list[str], k: int = 5):\n",
        "    \"\"\"\n",
        "    Plots performances of models in a bar chart.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    perfs : list[list[float]]\n",
        "        A list of accuracy results for each model\n",
        "    names : list[str]\n",
        "        The names of each of the models (in the same order as perfs)\n",
        "    k : int, default=5\n",
        "        The value of k used for cross-validation when producing the performances\n",
        "    \"\"\"\n",
        "    means = []\n",
        "    stds = []\n",
        "    for i, perf in enumerate(perfs):\n",
        "        mean = np.mean(perf)\n",
        "        means.append(mean)\n",
        "        stds.append(np.std(perf))\n",
        "        print(\"%s:\\t%.03f\" % (names[i], mean))\n",
        "    plt.bar(np.arange(len(means)), means, yerr=stds)\n",
        "    plt.xticks(np.arange(len(names)), names)\n",
        "    plt.ylabel(f\"Accuracy with {k} Folds\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hngtdUFUn0P6"
      },
      "source": [
        "### Step 7: Run the full workflow!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBiT5f1Bn34C"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "K_FOLD = 10\n",
        "raw_tweets, y = load_data(FILEPATH)\n",
        "\n",
        "X_preproc = preprocess(raw_tweets)\n",
        "bow_model = BOW_Classifier()\n",
        "basic_bow_accs = run_kfold_crossval(bow_model, X_preproc, y, k=K_FOLD)\n",
        "\n",
        "# here, we are going generate the \"most frequent class\" baseline based on the training data\n",
        "counts = Counter(y).values()\n",
        "mfc_baseline = [max(counts) / sum(counts)] * K_FOLD\n",
        "\n",
        "# plot the results!\n",
        "plot_perfs([mfc_baseline, basic_bow_accs], [\"MFC Baseline\", \"Basic BOW\"], k=K_FOLD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBN_jXx8fq4C"
      },
      "source": [
        "# Part 2: Improved Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqObqbbplLJ5"
      },
      "source": [
        "In this section, we are going to improve the preprocessing step, but otherwise keep the above workflow the same.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3sLallHnUJ7"
      },
      "source": [
        "### Loading Data with Spacy\n",
        "\n",
        "The cell block below will load the dataset into spacy_processed_docs, a list of spaCy docs (spacy.tokens.doc.Doc) that will be passed to the preprocess_part2 function below. Loading and parsing all the data using spaCy may take a while, so we've set it up to save the processed docs to a pickle file (saved in the same folder in your Google Drive where you put the dataset) that will be reloaded the next time(s) you run the cell.\n",
        "\n",
        "When debugging, you may find that having too many tweets will make debugging difficult. Feel free to change `NUM_TWEETS` to a smaller number while debugging, but make sure to change it back once finished.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llXP-yFMnUJ8"
      },
      "outputs": [],
      "source": [
        "spacy_doc: TypeAlias = spacy.tokens.doc.Doc\n",
        "\n",
        "NUM_TWEETS = 5000  # INFO: Feel free to change this to load in fewer tweets when debugging, but otherwise keep it at 5000\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "CACHE_PATH = f\"{FOLDER}/parsed_{NUM_TWEETS}_tweets.pickle\"\n",
        "\n",
        "if os.path.exists(CACHE_PATH):\n",
        "    print(f\"Loading parsed tweets from cache at {CACHE_PATH}\")\n",
        "    parsed_tweets = pickle.load(open(CACHE_PATH, \"rb\"))\n",
        "else:\n",
        "    # parse all the tweets with spacy\n",
        "    parsed_tweets = []\n",
        "    for i, r in enumerate(raw_tweets):\n",
        "        if i == NUM_TWEETS:\n",
        "            break\n",
        "        parsed_tweets.append(nlp(r))\n",
        "        if (i + 1) % 500 == 0:\n",
        "            print(f\"Processed {i + 1} out of {len(raw_tweets)}\", end=\"\\r\")\n",
        "    print(\"Processing complete\")\n",
        "    if CACHE_PATH is not None:\n",
        "        pickle.dump(parsed_tweets, open(CACHE_PATH, \"wb\"))\n",
        "\n",
        "print(f\"{len(parsed_tweets)} parsed tweets loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xKdQNr3gUUC"
      },
      "source": [
        "### Re-implement pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXbiOh3dnUJ8"
      },
      "source": [
        "Please implement `preprocessing_part2` to do the following things (not necessarily in this order):\n",
        "\n",
        "- lowercasing\n",
        "- lemmatization\n",
        "- remove stop words\n",
        "- remove punctuation and extra white space\n",
        "- use only top 1000 most frequent words, and replace the rest with \"\\<OOV\\>\"\n",
        "- replace numbers with \"\\<NUM\\>\"\n",
        "\n",
        "**Think about how each step affects the next one in the pipeline.** Specifically, one of these steps makes most sense to do last, so make sure your code reflects that.\n",
        "\n",
        "Your final feature matrix should have 1001 columns (1000 most frequent words and one OOV token).\n",
        "\n",
        "Use spaCy to do these things. You shouldn't need to import any additional libraries (but hint: we've imported the Counter library in a previous cell). You can explore the spaCy documentation [here](https://spacy.io/api/) and [here](http://spacy.pythonhumanities.com/01_02_linguistic_annotations.html#token-attributes).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtH1w0ekqSuo"
      },
      "outputs": [],
      "source": [
        "def preprocess_part2(parsed_tweets: list[spacy_doc], K: int = 1000) -> list[list[str]]:\n",
        "    \"\"\"\n",
        "    Preprocesses the spaCy-parsed tweets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    parsed_tweets : list[spacy_doc]\n",
        "        A list of tweets parsed by spaCy\n",
        "    K : number of most frequent words to include (default: 1000)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        A list of preprocessed tweets formatted as lists of tokens (lists of strings)\n",
        "    \"\"\"\n",
        "    # TODO: preprocessing steps outlined above\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPYyYr2YnUJ9"
      },
      "source": [
        "Now let's re-run the workflow and observe the difference in performance!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swJjfKjNZ0Jr"
      },
      "outputs": [],
      "source": [
        "X_preproc = preprocess_part2(parsed_tweets)\n",
        "bow_model = BOW_Classifier()\n",
        "better_preproc_accs = run_kfold_crossval(bow_model, X_preproc, y, k=K_FOLD)\n",
        "\n",
        "plot_perfs(\n",
        "    [mfc_baseline, basic_bow_accs, better_preproc_accs],\n",
        "    [\"MFC Baseline\", \"Basic BOW\", \"BOW+preproc\"],\n",
        "    k=K_FOLD,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMNapyQfmLHM"
      },
      "source": [
        "# Part 3: Improved Featurization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tBfdtKCuF2O"
      },
      "source": [
        "In this section, we will expand the featurize function to include more advanced and sophisticated features. You will now add n-grams with n values from 1 to 5 as features to (partially) preserve the order of the sentence.\n",
        "\n",
        "**Note:** You are not allowed to use a CountVectorizer to produce n-grams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ38t5IymRqN"
      },
      "outputs": [],
      "source": [
        "class Better_BOW(BOW_Classifier):\n",
        "    \"\"\"\n",
        "    A subclass of BOW_Classifier with a more complex featurization function.\n",
        "\n",
        "    All attribute and method names match those of BOW_Classifier.\n",
        "    \"\"\"\n",
        "\n",
        "    def featurize(self, preproc_X: np.ndarray[list[str]], is_test: bool = False) -> csr_matrix:\n",
        "        \"\"\"\n",
        "        Turns a list of preprocessed tweets into a bag-of-words matrix using n-grams up to 5-grams.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        preproc_X : list[list[str]]\n",
        "            A list of preprocessed tweets\n",
        "        is_test: bool, default=False\n",
        "            Whether featurization should be done using features learned during training (is_test=True)\n",
        "            or whether it should be done with features extracted from scratch using preproc_X (is_test=False)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        csr_matrix\n",
        "            A matrix with rows corresponding to tweets and columns corresponding to n-grams\n",
        "        \"\"\"\n",
        "        # TODO: Implement a binary BOW for 1-, 2-, 3-, 4-, and 5-grams\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQfVMtUUnUJ9"
      },
      "outputs": [],
      "source": [
        "X_preproc = preprocess_part2(parsed_tweets)\n",
        "better_bow = Better_BOW()\n",
        "better_feature_accs = run_kfold_crossval(better_bow, X_preproc, y, k=K_FOLD)\n",
        "\n",
        "plot_perfs(\n",
        "    [mfc_baseline, basic_bow_accs, better_preproc_accs, better_feature_accs],\n",
        "    [\"MFC Baseline\", \"Basic BOW\", \"BOW+preproc\", \"Better Features\"],\n",
        "    k=K_FOLD,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxD7CidRer20"
      },
      "source": [
        "# â— Conceptual Questions\n",
        "### Type your answer below each question (part) in the Colab cell below. ðŸ‘‡ [Don't miss this section!]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4w65qPFiiYS"
      },
      "source": [
        "1. What do the dimensions of the feature matrix correspond to?\n",
        "2. Find an example of a word that appears in the test set but never appears in the training set. What word did you find, and what is the tweet in which it occurred? **Include the code used to answer this question in the cell below.** Use any training and testing set that could be generated using K-Fold where `k=5`.\n",
        "3. What happens to that new word when the tweet is featurized at test time? i.e., describe what feature representation of the tweet that contains the unknown word would look like? **(Do not simply copy or write down the literal feature representation)**\n",
        "4. For each of the below preprocessing steps, indicate whether it is likely to result in more features or fewer, **all else being equal**. You can assume you are working with a very large corpus in which all words occur at least once. For each step, provide a one-sentence justification.\n",
        "\n",
        "    a. Lowercasing\n",
        "\n",
        "    b. Vocabulary Thresholding/Replacing Rare Words\n",
        "\n",
        "    c. Spelling out contractions (i.e., \"I'm\" -> \"I am\")\n",
        "\n",
        "    d. Spelling out abbreviations (i.e., \"RI\" -> \"Rhode Island\")\n",
        "\n",
        "    e. Part of Speech Tagging\n",
        "\n",
        "    f. Preserving typeface (i.e., indicating whether a word was italicized or bolded in the original text).\n",
        "\n",
        "5. A bag-of-words model does not take into account word order or number of words. Provide a context where a BOW model might fail for this reason."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLW-5VvfG41-"
      },
      "outputs": [],
      "source": [
        "# TODO: Copy code from other cells and combine them together along with some print statements to find the answer to conceptual question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NMsRDCMeV3u"
      },
      "source": [
        "# Submission Instructions\n",
        "\n",
        "Please submit this Colab file as **two** files to Gradescope:\n",
        "\n",
        "1.  A `.py` file: Click File > Download > Download .py\n",
        "2.  A `.ipynb` file with outputs: Click Runtime > Run All, and then after all outputs are complete, click File > Download > Download .ipynb\n",
        "\n",
        "Please ensure that the `.ipynb` contains actual function outputs (and not leftover print statements, for instance). We will run your `.ipynb` file; if our outputs don't match the outputs in your submitted file (within reason), you will receive a 0 on this assignment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VMdBqbCr8JM"
      },
      "source": [
        "Last updated: 13 Sept 2024\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}